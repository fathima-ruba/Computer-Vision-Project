{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCNwOwjRe4S"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrsMeVD-Vb0a"
      },
      "source": [
        "# We import necessary libraries. Numpy for math stuff, matplotlib for plotting graphs,\n",
        "# tensorflow to train our model, and tensorflow_datasets to load dataset.\n",
        "# We also import ResNet50 from keras for using a pre-trained model.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqtoVh52XaMB"
      },
      "source": [
        "**Load Dataset Info**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLaqHdvQXWAs"
      },
      "source": [
        "# Here, we are loading the EuroSAT dataset's info to understand what data we have.\n",
        "# This will tell us about the dataset like how many classes, types of data, etc.\n",
        "dataset = tfds.builder('eurosat')\n",
        "info = dataset.info\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXT9xX__XqEf"
      },
      "source": [
        "**Check Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dUWqD_cXkEd"
      },
      "source": [
        "# This line checks what kind of features our dataset got, like images and their labels.\n",
        "info.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Class Names**"
      ],
      "metadata": {
        "id": "IHel6e_1jxoN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-ZdD6skXxjy"
      },
      "source": [
        "#We make a list of all class names in our dataset so we know what categories we're dealing with.\n",
        "#loop through each class, convert number to string name, and add it to the list.\n",
        "class_names = []\n",
        "for i in range(info.features[\"label\"].num_classes):\n",
        "  class_names.append(info.features[\"label\"].int2str(i))\n",
        "\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List Dataset Splits**"
      ],
      "metadata": {
        "id": "ongVbFOPj9Rr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgCN3je0X_eZ"
      },
      "source": [
        "# This shows us all different parts of our dataset, like training, validation, and test splits.\n",
        "list(info.splits.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwsefp81YDvQ"
      },
      "source": [
        "eurosat has 10 classes with total of 27000 images under 1 split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC8EqdFhYODy"
      },
      "source": [
        "**Load Data Splits**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTE8xpJkYCxQ"
      },
      "source": [
        "# Here, we load our dataset into three parts: training, validation, and testing.\n",
        "# We split training data into 80%, validation into next 10%, and test into last 10%.\n",
        "(train, val, test) = tfds.load(\"eurosat/rgb\", split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Look at a Data Point**"
      ],
      "metadata": {
        "id": "I9WhwJKokhkN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl1KafT7Yj8C"
      },
      "source": [
        "# We take a single data point from train data to see what it looks like. This helps us understand our data better.\n",
        "datapoint = next(iter(train))\n",
        "datapoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPeiCc8hYsgB"
      },
      "source": [
        "This is a dictionary with 3 keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RggHc0HYvod"
      },
      "source": [
        "## **Show Some Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KLQWIsXYp8B"
      },
      "source": [
        "# We define a function to show a few images from our dataset. This makes it easy to see what kind of images we're working with.\n",
        "# Then, we show 9 images as examples.\n",
        "\n",
        "def show_images(dataset, num_images=9):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, (image, label) in enumerate(dataset.take(num_images)):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(class_names[label.numpy()])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_images(train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Training Parameters**"
      ],
      "metadata": {
        "id": "fBpnrAY4kxo0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jAMzcVGZHCb"
      },
      "source": [
        "# We're setting up some numbers to control our training, like how many times to go through data (epochs),\n",
        "# how many images to process at a time (batch size), and a buffer size for shuffling data.\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "IMAGE_SHAPE = [180, 180]\n",
        "NUM_CLASSES = info.features[\"label\"].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Dataset for Training**"
      ],
      "metadata": {
        "id": "BVrsWizmlAXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We loading the training part of dataset with extra info like names of classes.\n",
        "# This help us to understand what kind of pictures we dealing with in training.\n",
        "train_dataset, dataset_info = tfds.load('eurosat/rgb', split='train', with_info=True)\n",
        "\n",
        "# We get the names of all classes from dataset info. This names help us know categories like forest, sea, etc.\n",
        "class_names = dataset_info.features['label'].names\n",
        "\n",
        "# Initialize a dictionary to count occurrences of each class\n",
        "class_counts = {class_name:0 for class_name in class_names}\n",
        "\n",
        "for example in tfds.as_numpy(train_dataset):\n",
        "    class_counts[class_names[example['label']]] += 1\n",
        "\n",
        "# Plotting class distribution\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(range(len(class_counts)), list(class_counts.values()), tick_label=list(class_counts.keys()))\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Class Names')\n",
        "plt.ylabel('Number of Examples')\n",
        "plt.title('Class Distribution in EuroSAT Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j9Do1fCaFBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQEc5TuVZXK_"
      },
      "source": [
        "## **Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dB_wo_MZVf8"
      },
      "source": [
        "# Sometimes we need a random number between 0 and 1, maybe to decide if we gonna flip an image or not during data augmentation.\n",
        "tf.random.uniform(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dt95nokZjJR"
      },
      "source": [
        "@tf.function\n",
        "def prepare_training_data(datapoint):\n",
        "  input_image = tf.image.resize(datapoint[\"image\"], IMAGE_SHAPE)\n",
        "\n",
        "# We can use this random number in conditions. For example, if random number is more than 0.5, we might do some extra image processing. It makes our data augmentation more dynamic.\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    input_image = tf.image.random_flip_left_right(input_image)\n",
        "    input_image = tf.image.random_flip_up_down(input_image)\n",
        "    input_image = tf.image.random_brightness(input_image, max_delta=0.3)\n",
        "    input_image = tf.image.random_saturation(input_image, lower=0.75, upper=1.5)\n",
        "    input_image = tf.image.random_contrast(input_image, lower=0.75, upper=1.5)\n",
        "\n",
        "  input_image = preprocess_input(input_image)\n",
        "\n",
        "  return input_image, datapoint[\"label\"]\n",
        "\n",
        "def prepare_validation_data(datapoint):\n",
        "  input_image = tf.image.resize(datapoint[\"image\"], IMAGE_SHAPE)\n",
        "  input_image = preprocess_input(input_image)\n",
        "\n",
        "  return input_image, datapoint[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UokMG89Rastv"
      },
      "source": [
        "# Here, we preparing our training and validation data. We use a function for each to make sure data is ready for model.\n",
        "# 'num_parallel_calls' helps to speed up processing by doing multiple tasks at once.\n",
        "train = train.map(prepare_training_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "validation = val.map(prepare_validation_data)\n",
        "\n",
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_vTNSxbbQFK"
      },
      "source": [
        "## **Visualization of image after preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aekaPBhYbOyi"
      },
      "source": [
        "# We start by creating a big space to show our images, setting the size so everything looks big.\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(9):\n",
        "  ax = plt.subplot(3, 3, i+1)\n",
        "  for datapoint in tfds.as_numpy(train_dataset.take(1)):\n",
        "    plt.imshow(datapoint[0][0].astype('uint8'))\n",
        "    plt.title(class_names[datapoint[1][0]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrS8huicNBX"
      },
      "source": [
        "Without using uint8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_eqcu8ob_Kc"
      },
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(4):\n",
        "  ax = plt.subplot(2, 2, i+1)\n",
        "  for datapoint in tfds.as_numpy(train_dataset.take(1)):\n",
        "    plt.imshow(datapoint[0][0])\n",
        "    plt.title(class_names[datapoint[1][0]])\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJeU7sP_cfb8"
      },
      "source": [
        "## **Building model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzIxAaWzcavR"
      },
      "source": [
        "# First, we setting up ResNet50 model. We telling it the size of images it should expect and to use pre-trained 'imagenet' weights.\n",
        "# 'include_top=False' means we not using the last part of model, cause we going to add our own layers for our specific task.\n",
        "resnet = ResNet50(input_shape=IMAGE_SHAPE+[3], weights='imagenet', include_top=False)\n",
        "\n",
        "# Here, we go through each layer in ResNet model we just setup, and tell them not to change during training.\n",
        "# This because they already learned a lot from 'imagenet', and we wanna keep that knowledge.\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Now, we start adding our own layers on top of ResNet to make it suitable for our task.\n",
        "# 'GlobalAveragePooling2D' helps to reduce the dimensions but keeps important stuff.\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(resnet.output)\n",
        "# 'Flatten' makes our data into a single list to be easier to work with in next layers.\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "# 'Dense' is a normal layer where we can learn stuff, we use 512 units here and 'relu' to decide what to activate.\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "# Last layer is also 'Dense' where we decide between our classes. 'softmax' makes sure output is in probabilities.\n",
        "predicition = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=resnet.input, outputs=predicition)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF_NvafedMea"
      },
      "source": [
        "# Now, we telling our model how to learn. This step is like setting rules for how model should improve itself.\n",
        "# 'loss' is what tells model how wrong its guesses are. We use 'SparseCategoricalCrossentropy' which is good for when we have many classes.\n",
        "# 'from_logits=False' means our model outputs are already like probabilities, so no need to convert them.\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam', # 'adam' is a smart way to change model to make it better. It adjusts itself as model learns.\n",
        "    metrics=['accuracy'] # We wanna know how often model guesses right, so we check 'accuracy'.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePHBDy9ddju"
      },
      "source": [
        "# We calculating how many steps to do in one epoch when training.\n",
        "# Since we using 80% of data for training, we multiply total number of examples by 0.8.\n",
        "# Then, we divide by 'BATCH_SIZE' to know how many batches of data we got. We use '//' to get whole number.\n",
        "STEPS_PER_EPOCH = int(info.splits[\"train\"].num_examples * 0.8)//BATCH_SIZE\n",
        "\n",
        "# Similar way, we calculate steps for validation. But this time, we use 10% of data ('0.1').\n",
        "# Again, divide by 'BATCH_SIZE' to get how many batches we need to go through during validation.\n",
        "VALIDATION_STEPS = int(info.splits[\"train\"].num_examples * 0.1)//BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJpXxnDgdtMe"
      },
      "source": [
        "# Now, we start the actual training of model. We telling it to look at our training data and learn from it.\n",
        "# 'epochs = NUM_EPOCHS' means how many times model will go through all the training data.\n",
        "# 'steps_per_epoch' is how many batches of images it will look at in one go before it says one epoch is done.\n",
        "# This is cause we might not want to use all data in one epoch, especially if we got lots of it.\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = NUM_EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=VALIDATION_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MELiVZ7fd74K"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Now, we take our training history which got all sorts of numbers like accuracy and loss over time,\n",
        "# and we turning it into a pandas DataFrame. This is like making it into a neat table.\n",
        "pd.DataFrame(\n",
        "    history.history # 'history.history' is where TensorFlow keeps track of how model did after each epoch.\n",
        ").plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-IQ0rzggIER"
      },
      "source": [
        "## **Evaluating results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j805qpLh2A2"
      },
      "source": [
        "# Now, we getting our test data ready. This is the data we gonna use to finally check how good our model is after all the training.\n",
        "test_dataset = test.map(prepare_validation_data)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTx490gNivPI"
      },
      "source": [
        "# Here we telling our model to check how well it does on test data.\n",
        "# It will give us back numbers telling how accurate it is and how much error (loss) it got.\n",
        "model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKm6UgGpix0B"
      },
      "source": [
        "# We making a big space to show a bunch of test images and our model's guesses.\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i, datapoint in enumerate(tfds.as_numpy(test.take(25))):\n",
        "  ax = plt.subplot(5, 5, i+1)\n",
        "  plt.imshow(datapoint[\"image\"])\n",
        "  image = tf.image.resize(datapoint[\"image\"], IMAGE_SHAPE)\n",
        "  image = preprocess_input(image)\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "\n",
        "  # Now, we make our model predict what it thinks this image is.\n",
        "  # If the model's guess (highest probability class) matches the true label, we show the name in green.\n",
        "  # If it's wrong, we show the name in red.\n",
        "\n",
        "  if datapoint[\"label\"] == np.argmax(model.predict(image)):\n",
        "    plt.title(class_names[np.argmax(model.predict(image))], color=\"green\")\n",
        "  else:\n",
        "    plt.title(class_names[np.argmax(model.predict(image))], color=\"red\")\n",
        "\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLrSY7fIjn9z"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# This function helps us to figure out what picture is showing by using our trained model.\n",
        "def classify_image(image_path, model, preprocess_input, class_names):\n",
        "    \"\"\"\n",
        "    Classifies an input image using the provided model.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: Path to the image to be classified.\n",
        "    - model: Trained model to use for prediction.\n",
        "    - preprocess_input: Preprocessing function to apply to the image before prediction.\n",
        "    - class_names: List of class names corresponding to model output.\n",
        "\n",
        "    Returns:\n",
        "    - Predicted class name.\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.preprocessing import image\n",
        "    import numpy as np\n",
        "    # First, we load the image so we can show it to you.\n",
        "    img = image.load_img(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')  # Hide the axis\n",
        "    plt.show()\n",
        "\n",
        "    # Now we make the picture the right size and do the magic preprocessing.\n",
        "    img = image.load_img(image_path, target_size=(180, 180))  # Adjusted to match model's expected input dimensions\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_image = preprocess_input(img_array_expanded_dims)\n",
        "\n",
        "    # Predict the class\n",
        "    predictions = model.predict(preprocessed_image)\n",
        "    predicted_class = class_names[np.argmax(predictions)]\n",
        "\n",
        "    return predicted_class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we assuming you already got everything set up: the model, how to make pictures ready, and the list of things it can guess.\n",
        "\n",
        "# Now, we gonna try out our model with some pictures to see what it thinks they are.\n",
        "\n",
        "# We got a picture saved at '/content/verification image 1.jpg'. We wanna know what our model thinks it is.\n",
        "image_path = '/content/verification image 1.jpg'\n",
        "predicted_class = classify_image(image_path, model, preprocess_input, class_names)\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class}\")\n",
        "\n",
        "\n",
        "image_path = '/content/verification image 2.jpg'\n",
        "predicted_class = classify_image(image_path, model, preprocess_input, class_names)\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class}\")\n",
        "\n",
        "image_path = '/content/verification image 3.jpg'\n",
        "predicted_class = classify_image(image_path, model, preprocess_input, class_names)\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class}\")"
      ],
      "metadata": {
        "id": "XUws3qY7T4WN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}